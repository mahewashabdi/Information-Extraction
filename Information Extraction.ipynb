{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC-LZ20S_WUr"
   },
   "source": [
    "# Assignment : Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9xqCFJBv_WUt"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZNmDWxj-V-J"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### For subsequent tasks in this assignment, you will work with the documents in `football_players.txt` to perform various information extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V9YE4n6u7olU"
   },
   "outputs": [],
   "source": [
    "# Download the text file (uncomment the line below in this cell, if not already downloaded from Blackboard)\n",
    "#!curl \"https://ideone.com/plain/OvwDXZ\" > football_players.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpSaij2b73Vj"
   },
   "source": [
    " Read all the documents from `football_players.txt` into a list called `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qteh89cs7q4x"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "lines = open('5810556.txt',encoding=\"utf8\").readlines()\n",
    "for i in lines:\n",
    "    docs.append(i)\n",
    "\n",
    "## Docs has the file football_player.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCEJrJ-p_WU1"
   },
   "source": [
    "## Task 2\n",
    "Write a function that takes a document and returns a list of sentences with part-of-speech tags.\n",
    "\n",
    "Please keep in mind that the expected output is a list within a list as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U3MCJIcR_WU2"
   },
   "outputs": [],
   "source": [
    "\"\"\"Function for returning sentence where each word is Pos tagged. All the three task mentioned above performed inside the function\n",
    "and returned document is a pos tagged document. Input to the function must be a  string\"\"\"\n",
    "\n",
    "def preprocess_postag(document):\n",
    "    pos=[]\n",
    "    try:\n",
    "        sentences=nltk.sent_tokenize(document)   ##converting the string in to list of sentences (sentence segmentation)\n",
    "        words=[nltk.word_tokenize(sent) for sent in sentences]   ## tokenization of the sentence is done\n",
    "        for j in words:\n",
    "               pos.append(nltk.pos_tag(j)) ##pos_tagged words are returned \n",
    "        return pos\n",
    "    except:\n",
    "        print('Input should be a string')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E04CUNb_WU6"
   },
   "source": [
    "Run the cell below to verify your result for the second sentence in the first document.\n",
    "Expected output: \n",
    "`[('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('forward', 'NN'), ('and', 'CC'), ('serves', 'NNS'), ('as', 'IN'), ('captain', 'NN'), ('for', 'IN'), ('Portugal', 'NNP'), ('.', '.')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R30taRgf_WU6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('forward', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('serves', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('captain', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Portugal', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"first_doc is the first document in the given file.\"\"\"\n",
    "\n",
    "first_doc = docs[0]\n",
    "tagged_sentences = preprocess_postag(first_doc)  ##calling function to which returns tagged word.\n",
    "tagged_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYTwrZId_WU_"
   },
   "source": [
    "## Task 3 \n",
    "Write a function that takes a list of tokens with POS tags for a sentence and returns a list of named entities (NE). \n",
    "\n",
    "Hint: Use `binary = True` while calling NE chunk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5fC0iqJJ_WU_"
   },
   "outputs": [],
   "source": [
    "\"\"\"This function identifies the Named Entity chunks from the pos tagged sentences passed. Passing binary = True results in a \n",
    "named entities tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE.\"\"\"\n",
    "\n",
    "def find_named_entities(sent):\n",
    "    \n",
    "    \"\"\"Pos tagged sentence is passed in the function to return all the named entities\"\"\"\n",
    "        \n",
    "    try:\n",
    "        tree=nltk.ne_chunk(sent,binary=True)       ##The named entity tagged as NE is recoginized using ne_chunk\n",
    "\n",
    "        named_entities = []                       ##list to store all named entities\n",
    "\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label()=='NE':           ##if the subtree is a named entity then appending the leaves of the subtree         \n",
    "                entity=''\n",
    "                for leaf in subtree.leaves():\n",
    "                    entity=entity+leaf[0]+\" \"\n",
    "                named_entities.append(entity.strip())\n",
    "\n",
    "        return named_entities\n",
    "    except:\n",
    "        print('Please make sure the input enter is the list of tokens with POS tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td5yJ8cgFScx"
   },
   "source": [
    "Run the cell below to verify your result for the first sentence in the first document.\n",
    "Expected output: `['Cristiano Ronaldo', 'Santos Aveiro', 'ComM', 'GOIH', 'Portuguese', 'Portuguese', 'Spanish', 'Real Madrid', 'Portugal']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FijjdAPWFsp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cristiano Ronaldo', 'Santos Aveiro', 'ComM', 'GOIH', 'Portuguese', 'Portuguese', 'Spanish', 'Real Madrid', 'Portugal']\n"
     ]
    }
   ],
   "source": [
    "tagged_sent_doc1 = preprocess_postag(docs[0])    ## tagged sentences of document 1 is returned\n",
    "doc1_sent1=find_named_entities(tagged_sent_doc1[0])   ## named entity in first sentence of document 1\n",
    "print(doc1_sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHMp7xtK_WVE"
   },
   "source": [
    "## Task 4 \n",
    "\n",
    "Implement the `find_all_named_entities` function below to find **all** NEs in a given document.\n",
    "\n",
    "Hint: Use `find_named_entities` implemented above for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Input is the document of one string containing\"\"\"\n",
    "\n",
    "def find_all_named_entities(doc):\n",
    "    named_entity=[]\n",
    "    tagged=preprocess_postag(doc)\n",
    "    for sent in tagged:\n",
    "        ne=find_named_entities(sent)\n",
    "        named_entity.extend(ne)\n",
    "    \n",
    "    return named_entity\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdM0-LZlJy4u"
   },
   "source": [
    "How many named entities did you find in the first document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total no. of named entities in the first document is  56\n"
     ]
    }
   ],
   "source": [
    "named_ent=len(find_all_named_entities(docs[0]))\n",
    "\n",
    "print(\"The total no. of named entities in the first document is \",named_ent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2AzD9MVNIx2"
   },
   "source": [
    "## Task 5 \n",
    "\n",
    "Find named entities across **all** documents in `football_players.txt`, and save the result into a single flat list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YULMcK1-NSR9"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "def all_named_ent(file):\n",
    "    all_named_entities = []\n",
    "    for doc in file:\n",
    "        all_named_entities.extend(find_all_named_entities(doc))\n",
    "    \n",
    "    return all_named_entities\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaM9Cs9zNGM2"
   },
   "source": [
    "How many named entities did you find across all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jCNIrC_SNpHQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total no. of named enities across the whole document contains 380 values\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "ne_list=all_named_ent(docs)   ## list of all the named entities across the whole document\n",
    "\n",
    "print(\"The total no. of named enities across the whole document contains \"+str(len(ne_list))+\" values\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7-ma9lJ_WVJ"
   },
   "source": [
    "## Task 6 \n",
    "\n",
    "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
    "\n",
    "Hint: Use the `re.compile()` function to create the extraction patterns.\n",
    "\n",
    "Reference: https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tbaFyiah_WVK"
   },
   "outputs": [],
   "source": [
    "def name_of_the_player(doc):\n",
    "    \n",
    "    \"\"\"In the function, since the name of the player is occuring along with word born so using regex we are extracting the complete\n",
    "    sentence and then using nltk function, POS tagging of the senetnce is done.In order to create a NP-chunker,a chunker grammer is defined and using this grammar\n",
    "    chunk parseris defined which results in a tree. Using subtree.leaves we extract the name of the player\"\"\"\n",
    "    \n",
    "    name=[]\n",
    "    noun_phrases = []\n",
    "    pattern1=re.compile(r'.*\\bborn\\b.+?(?=\\))') ##pattern to extract the line in whih name is occuring along with born\n",
    "    names=pattern1.findall(doc)\n",
    "    token=nltk.word_tokenize(names[0])\n",
    "    pos = nltk.pos_tag(token)\n",
    "    grammar= r\"\"\"{NP: {<NNP><NN|NNP|,>*<NNP>}\"\"\"    ##Grammar for chunking is defined\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tree = chunker.parse(pos)   ##The result is a tree after the parsing with the grammar defined\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        \n",
    "        noun_phrases.append(subtree.leaves())           \n",
    "    print(noun_phrases)\n",
    "    ##The noun-phrase is a list tuples with name and POS tags so we extract the name of the player in following name list\n",
    "    name=[tuple_[0] for tuple_ in noun_phrases[1]]\n",
    "    name=\" \".join(name)   \n",
    "    if name.find(','):    ## condition to check if ',' is present in the name of the player then remove it from player name \n",
    "        name1=name.replace(',',\"\")\n",
    "        return name1\n",
    "    else:\n",
    "        return name\n",
    "    \n",
    "def country_of_origin(doc):\n",
    "    \n",
    "    \"\"\"Pattern matches word character which is occuring along with national team and using group capture, \n",
    "    country of origin is extracted and returned by the function\"\"\"\n",
    "    \n",
    "    pattern=re.compile(r'([A-Z][a-z]+)\\s\\bnational team')\n",
    "    c_list=pattern.findall(doc)\n",
    "    country= list(set(c_list))\n",
    "    return country[0]\n",
    "\n",
    "def date_of_birth(doc):\n",
    "    \n",
    "    \"\"\"regex pattern for matching word characters that occur with born and using group the date of birth is extracted. The day\n",
    "    format in the capturing group should be 1 digit or 2 digit that is why \\d{1,2} has been used to capture wherever the 2 digits are \n",
    "    occuring along with sentence born followed by the name of month. For year the number of digits should be 4, therefore \\d{4} is used for capturing where 4 digits are occuring along with word born.\"\"\" \n",
    "    \n",
    "    dob=re.compile(r'born\\b\\s(\\d{1,2}\\s\\w+\\s\\d{4})')\n",
    "    date=dob.findall(doc)\n",
    "    return date[0]\n",
    "\n",
    "    \n",
    "def team_of_the_player(doc):\n",
    "    \n",
    "    \"\"\"Some of the players doesn't have national team information therefore assumed that player plays for his country of origin (might not be true in every case)\"\"\"\n",
    "    \n",
    "    \"\"\"regex for capturing club names since the named entity is preceded by word 'club' and after that club name(named entity) is present.\n",
    "    I am trying to capture the complete sentence after word 'club' in group using the regex pattern. The sentence captured in the group are then passed to\n",
    "    find_all_named_entities function to extract all the named entities \"\"\"\n",
    "    \n",
    "    regex=re.compile(r'[A-Z][a-z]{3,} \\bclub(.*?)\\.')\n",
    "    doc1=regex.findall(doc)\n",
    "    ne=[]\n",
    "    if doc1:       ##in some document the there are two sentence where the same pattern is there\n",
    "        for i in doc1:\n",
    "            \"\"\"All the named entity are getting captured but in document4, all the club name are captured but for club 'Paris Saint-Germain', only it is capturing only Paris for that\"\"\"\n",
    "            x=find_all_named_entities(i)     \n",
    "            ne.extend(x)\n",
    "            \n",
    "        \"\"\"if the country of original is present in named entity then concatenating 'national team' to it and if country name is not present then\n",
    "        append country name with national team as player plays in his national team (assumption)\"\"\"\n",
    "        \n",
    "        if country_of_origin(doc) in ne:      \n",
    "            ne1 = [i.replace(country_of_origin(doc), country_of_origin(doc)+' national team') for i in ne if country_of_origin(doc) in ne]\n",
    "            return ne1\n",
    "        else: \n",
    "            ne.append(country_of_origin(doc)+' national team')\n",
    "            return ne\n",
    "    else:   ##if no pattern is captured return the country name as the team name(assumption, possible might not be true)\n",
    "        return country_of_origin(doc)+' national team'    \n",
    "    \n",
    "    \n",
    "def position_of_the_player(doc):    \n",
    "    \n",
    "    \"\"\"The positions are occuring along with different words like ['is a','as a','as an']. To capture all the positions\n",
    "    multiple patterns are used and through groups we are trying to capture the position of the player. Positive lookahead has been\n",
    "    used to look for positions occuring after the words in the list mentioned above.\"\"\"\n",
    "\n",
    "    pattern1=re.compile(r'(?<=\\bis a|\\bas a)\\s([a-z]+\\s)for.[A-Z]+[a-z]+')\n",
    "    pattern2=re.compile(r'(?<=\\bis a)( [a-z]+)[\\w+\\s+]+for [A-Z][a-z]+')\n",
    "    pattern3=re.compile(r'(?<=\\bas a) (\\w+\\s[a-z]{5,})')\n",
    "    pattern4=re.compile(r'(?<=\\bas an)( \\w*ing\\s[a-z]+[\\w+\\s]+)\\.*')\n",
    "    pattern5=re.compile(r'(?<=\\bcareer as a) ([a-z]+)')\n",
    "    \n",
    "    list1=pattern1.findall(doc)\n",
    "    list2=pattern2.findall(doc)\n",
    "    list3=pattern3.findall(doc)\n",
    "    list4=pattern4.findall(doc)\n",
    "    list5=pattern5.findall(doc)\n",
    "    \n",
    "    position=list1+list2+list3+list4+list5\n",
    "        \n",
    "    if re.match(r'(?=.*\\bor\\b)', position[0]):   ##Since there are multiple position separated by 'or' so using re.split to return the positions in the list\n",
    "        position=re.split(\"or |[^a-zA-Z ]+\",position[0])\n",
    "        position=[i.strip() for i in position]\n",
    "    else:\n",
    "        position=position[0].strip()\n",
    "        \n",
    "        \n",
    "    return position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-CNrMM5_WVO"
   },
   "source": [
    "Execute the cell below to verify the `date_of_birth` function for the third player. Expected output `5 February 1992`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jpeKE1u9_WVP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 February 1992\n"
     ]
    }
   ],
   "source": [
    "print(date_of_birth(docs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Lionel', 'NNP'), ('Andrés', 'NNP'), ('``', '``'), ('Leo', 'NNP'), (\"''\", \"''\"), ('Messi', 'NNP'), ('(', '('), ('Spanish', 'JJ'), ('pronunciation', 'NN'), (':', ':'), ('[', 'JJ'), ('ljoˈnel', 'NNS'), ('anˈdɾes', 'VBP'), ('ˈmesi', 'JJ'), (']', 'NNP'), (';', ':'), ('born', 'VBN'), ('24', 'CD'), ('June', 'NNP'), ('1987', 'CD')], [('Lionel', 'NNP'), ('Andrés', 'NNP')]]\n",
      "Lionel Andrés\n"
     ]
    }
   ],
   "source": [
    "print(name_of_the_player(docs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3VtWxBr_WVZ"
   },
   "source": [
    "## Task 6 \n",
    "Identify one other relation (besides team and player) and write a function to extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Relation of debut age and debut has been derived using this function\"\"\"\n",
    "\n",
    "\n",
    "def debut_age_year(doc):\n",
    "    Relation={}   \n",
    "    \n",
    "    \"\"\"Positive look ahead has been used in forming regex as the age of the player is coming after 'age' in every document.Age is occuring in 2 digits and using positive lookahead\n",
    "    trying to capture the age in group. The year of the debut is occuring along with word 'debut' and it is 4 digits in size, so capturing\n",
    "    the year using group again where similar pattern is observed.) \"\"\"\n",
    "    \n",
    "    age=re.compile(r'(?<=debut)[\\s\\w+\\s,]+(?<=age)[a-z+\\s+]*([\\d]{2})') ##regex for capturing the debut age of the player \n",
    "    year=re.compile(r'debut[\\s\\w+\\s]*?(\\d{4}?)')   ##Regex for capturing the debut year of the player\n",
    "    group_age=age.findall(doc)\n",
    "    group_year=year.findall(doc)\n",
    "    \n",
    "    if group_age:    ##certain documents doesn't have age mentioned \n",
    "        Relation['Age']=group_age[0] ##for document3 debut age are returned, the second age is the international debut age so considering first element of the group\n",
    "    else:\n",
    "        Relation['Age']='not mentioned'\n",
    "        \n",
    "    if group_year:  ##certain documents doesn't have year mentioned so possible that the list coulb be empty list\n",
    "        Relation['Debut Year']=group_year[0] ##Due to present of multiple debut year at different level, we are considering first group which is capturing the debut year in football correctly\n",
    "    else:\n",
    "        Relation['Debut Year']='not mentioned'\n",
    "        \n",
    "    return Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': '18', 'Debut Year': '2003'}\n"
     ]
    }
   ],
   "source": [
    "print(debut_age_year(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2.ipynb",
   "provenance": [
    {
     "file_id": "1EXXdimBbQY8nnqIs5hBXdG68r2hsk7HS",
     "timestamp": 1604940588321
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
